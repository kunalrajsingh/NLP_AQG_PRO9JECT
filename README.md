This project explores the development of an Automatic Question Generator using deep learning models, specifically BERT (Bidirectional Encoder Representations from Transformers) and T5 (Text-To-Text Transfer Transformer), to create contextually relevant questions from input content. Automated question generation is essential in applications such as educational tools, conversational AI, and content analysis, where context-aware questions enhance user engagement and comprehension. Leveraging both BERT’s contextual encoding and T5’s text generation capabilities, the model produces questions that are syntactically accurate and semantically aligned with the given text, suitable for applications in e-learning, quizzes, and virtual assistants. The integration of BERT and T5 enables a more nuanced understanding and generation process, resulting in high-quality, coherent questions. This project advances the field of NLP by providing an efficient, adaptable tool for question generation, with potential benefits in adaptive learning technologies and interactive content.
